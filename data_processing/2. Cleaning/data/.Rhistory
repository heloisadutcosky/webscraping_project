z = rnorm(20*1000)
zSq = z*z
chimat = matrix(zSq, 20)
chiSqvec = colSums(chimat)
fvec = (chiSqvec / 10) / (chiSqvec2 / 20)
# comparação dois qui-quadrados
z = rnorm(10*1000)
zSq = z*z
chimat = matrix(zSq, 10)
chiSqvec = colSums(chimat)
z = rnorm(20*1000)
zSq = z*z
chimat = matrix(zSq, 20)
chiSqvec2 = colSums(chimat)
fvec = (chiSqvec / 10) / (chiSqvec2 / 20)
fvec
hist(fvec)
hist(fvec, add = T)
density(fvec)
plot(density(fvec))
plot(density(rf(1000, df1 = 19))
plot(density(rf(1000, df1 = 19)))
density(rf(1000, df1 = 19))
plot(density(fvec), add = T)
plot(density(fvec), add = T)
plot(density(fvec))
density(fvec)
plot(density(fvec))
density(rf(1000, df1 = 19))
plot(density(rf(1000, df1 = 10, df2 = 20)))
plot(density(fvec))
plot(density(rf(1000, df1 = 10, df2 = 20)))
plot(density(fvec))
plot(density(rf(1000, df1 = 10, df2 = 20)), add = T)
plot(density(rf(1000, df1 = 10, df2 = 20))
plot(density(fvec))
plot(density(rf(1000, df1 = 10, df2 = 20)))
plot(density(fvec))
plot(density(rf(1000, df1 = 10, df2 = 20)))
plot(density(fvec))
plot(density(rf(1000, df1 = 10, df2 = 20)))
fvec = (chiSqvec / 10) / (chiSqvec2 / 20)
fvec
chiSqvec / 10
chiSqvec2 / 20
?rnorm
# distribuição normal de 12000 valores
U = runif(12*1000)
# matriz a partir da distribuição uniforme
mat = matrix(data = U, nrow = 12)
# soma das colunas da matriz
normvec = colSums(mat)
# frequencia da soma das colunas => se assemelha a uma distribução normal
hist(normvec)
# qui-quadrado
z = rnorm(10*1000) # => distribuição normal de 10 pontos
hist(z)
# qui-quadrado
z = rnorm(10*1000) # => distribuição normal de 10 pontos
zSq = z*z # => Qui-quadrado -> variância da distribuição normal com média de z = 0
chimat = matrix(zSq, 10) # => distribuição dos valores em uma matriz de 10 linhas
chiSqvec = colSums(chimat) # => agrupamento dos 10 pontos
hist(chiSqvec, add = F)
hist(rchisq(1000,10), add = T, col = "blue")
# qui-quadrado
z = rnorm(10*1000) # => distribuição normal de 10 pontos
zSq = z*z # => Qui-quadrado -> variância da distribuição normal com média de z = 0
chimat = matrix(zSq, 10) # => distribuição dos valores em uma matriz de 10 linhas
chiSqvec = colSums(chimat) # => agrupamento dos 10 pontos
hist(chiSqvec, add = F)
hist(rchisq(1000,10), add = T, col = "blue")
?t.statistic                                                #the t-statistic
t.statistic = (mean(heights) - 68)/(sd(heights)/sqrt(100)) #Manually calculating
t.statistic                                                #the t-statistic
#function.
>t.test
#function.
?t.test
##################################
#####Visualizing Missing Data#####
##################################
install.packages('VIM')
library(VIM) #For the visualization and imputation of missing values.
help(sleep) #Inspecting the mammal sleep dataset.
sleep
summary(sleep) #Summary information for the sleep dataset.
sapply(sleep, sd) #Standard deviations for the sleep dataset; any issues?
VIM::aggr(sleep) #A graphical interpretation of the missing values and their
install.packages('mice')
library(mice) #Load the multivariate imputation by chained equations library.
dim(sleep)
sum(is.na(sleep))
?md.pattern
mice::md.pattern(sleep) #Can also view this information from a data perspective.
dim(sleep)
sum(is.na(sleep))
?md.pattern
mice::md.pattern(sleep) #Can also view this information from a data perspective.
dim(sleep)
sum(is.na(sleep))
?md.pattern
library(mice) #Load the multivariate imputation by chained equations library.
install.packages('mice')
library(mice) #Load the multivariate imputation by chained equations library.
dim(sleep)
sum(is.na(sleep))
?md.pattern
mice::md.pattern(sleep) #Can also view this information from a data perspective.
mice::md.pattern(sleep, F) #Can also view this information from a data perspective.
install.packages('nloptr')
library(mice) #Load the multivariate imputation by chained equations library.
dim(sleep)
sum(is.na(sleep))
?md.pattern
mice::md.pattern(sleep, F) #Can also view this information from a data perspective.
mice::md.pattern(sleep) #Can also view this information from a data perspective.
###############################
#####Mean Value Imputation#####
###############################
#Creating a dataset that has missing values.
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10)))
missing.data
mean(missing.data$x2, na.rm = TRUE) #Mean of x2 prior to imputation.
sd(missing.data$x2, na.rm = TRUE) #Standard deviation of x2 prior to imputation.
cor(missing.data, use = "complete.obs") #Correlation prior to imputation.
#Mean value imputation method 1.
missing.data$x2[is.na(missing.data$x2)] = mean(missing.data$x2, na.rm=TRUE)
missing.data
cor(missing.data, use = "complete.obs") #Correlation prior to imputation.
mean(missing.data$x2, na.rm=TRUE)
missing.data$x2[is.na(missing.data$x2)]
#Mean value imputation method 3.
install.packages('caret')
library(caret)
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
pre = caret::preProcess(missing.data, method = "medianImpute")
missing.data = predict(pre, missing.data)
missing.data
pre
### Why Caret?
## 1. Maintain the structure of train - predict as other machine learning procedure.
##    This is particularly important when impute for future observation
## 2. Can be collected with other preprocesses, as below:
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
pre = preProcess(missing.data, method = c("scale", "medianImpute"))
missing.data = predict(pre, missing.data)
missing.data
pre = preProcess(missing.data, method = c("scale", "medianImpute"))
missing.data = predict(pre, missing.data)
missing.data
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
pre = preProcess(missing.data, method = c("center","scale", "medianImpute"))
missing.data = predict(pre, missing.data)
missing.data
## manual scale
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
scaled = mapply(FUN = '/',missing.data,sapply(missing.data, function(x) {sd(x,na.rm=T)}))
scaled
#Mean value imputation method 4.
library(Hmisc) #Load the Harrell miscellaneous library.
#Mean value imputation method 4.
install.packages('Hmisc')
library(Hmisc) #Load the Harrell miscellaneous library.
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
library(Hmisc) #Load the Harrell miscellaneous library.
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
imputed.x2 = impute(missing.data$x2, mean) #Specifically calling the x2 variable.
imputed.x2
View(missing.data)
summary(imputed.x2) #Summary information for the imputed variable.
is.imputed(imputed.x2) #Boolean vector indicating imputed values.
missing.data$x2 = imputed.x2 #Replacing the old vector.
mean(missing.data$x2) #Mean of x2 after imputation.
sd(missing.data$x2) #Standard deviation of x2 after imputation.
cor(missing.data, use = "complete.obs") #Correlation afterto imputation.
plot(missing.data) #What are some potential problems with mean value imputation?
##################################
#####Simple Random Imputation#####
##################################
#Recreating a dataset that has missing values.
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10)))
missing.data
mean(missing.data$x2, na.rm = TRUE) #Mean of x2 prior to imputation.
sd(missing.data$x2, na.rm = TRUE) #Standard deviation of x2 prior to imputation.
cor(missing.data, use = "complete.obs") #Correlation prior to imputation.
set.seed(0)
imputed.x2 = impute(missing.data$x2, "random") #Simple random imputation using the
#impute() function from the Hmisc package.
imputed.x2
summary(imputed.x2) #Summary information for the imputed variable.
is.imputed(imputed.x2) #Boolean vector indicating imputed values.
missing.data$x2 = imputed.x2 #Replacing the old vector.
mean(missing.data$x2) #Mean of x2 after imputation.
sd(missing.data$x2) #Standard deviation of x2 after imputation.
cor(missing.data, use = "complete.obs") #Correlation afterto imputation.
plot(missing.data) #What are some potential problems with mean value imputation?
##################################
#####Simple Random Imputation#####
##################################
#Recreating a dataset that has missing values.
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10)))
missing.data
mean(missing.data$x2, na.rm = TRUE) #Mean of x2 prior to imputation.
sd(missing.data$x2, na.rm = TRUE) #Standard deviation of x2 prior to imputation.
cor(missing.data, use = "complete.obs") #Correlation prior to imputation.
set.seed(0)
imputed.x2 = impute(missing.data$x2, "random") #Simple random imputation using the
#impute() function from the Hmisc package.
imputed.x2
summary(imputed.x2) #Summary information for the imputed variable.
is.imputed(imputed.x2) #Boolean vector indicating imputed values.
missing.data$x2 = imputed.x2 #Replacing the old vector.
mean(missing.data$x2) #Mean of x2 after imputation.
sd(missing.data$x2) #Standard deviation of x2 after imputation.
cor(missing.data, use = "complete.obs") #Correlation afterto imputation.
plot(missing.data) #What are some potential problems with mean value imputation?
#############################
#####K-Nearest Neighbors#####
#############################
#Recreating a dataset that has missing values.
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10)))
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
imputed.x2 = impute(missing.data$x2, mean) #Specifically calling the x2 variable.
imputed.x2
summary(imputed.x2) #Summary information for the imputed variable.
is.imputed(imputed.x2) #Boolean vector indicating imputed values.
missing.data$x2 = imputed.x2 #Replacing the old vector.
mean(missing.data$x2) #Mean of x2 after imputation.
sd(missing.data$x2) #Standard deviation of x2 after imputation.
cor(missing.data, use = "complete.obs") #Correlation afterto imputation.
plot(missing.data) #What are some potential problems with mean value imputation?
##################################
#####Simple Random Imputation#####
##################################
#Recreating a dataset that has missing values.
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10)))
missing.data
mean(missing.data$x2, na.rm = TRUE) #Mean of x2 prior to imputation.
sd(missing.data$x2, na.rm = TRUE) #Standard deviation of x2 prior to imputation.
cor(missing.data, use = "complete.obs") #Correlation prior to imputation.
set.seed(0)
imputed.x2 = impute(missing.data$x2, "random") #Simple random imputation using the
#impute() function from the Hmisc package.
imputed.x2
summary(imputed.x2) #Summary information for the imputed variable.
is.imputed(imputed.x2) #Boolean vector indicating imputed values.
missing.data$x2 = imputed.x2 #Replacing the old vector.
mean(missing.data$x2) #Mean of x2 after imputation.
sd(missing.data$x2) #Standard deviation of x2 after imputation.
cor(missing.data, use = "complete.obs") #Correlation afterto imputation.
plot(missing.data) #What are some potential problems with mean value imputation?
#############################
#####K-Nearest Neighbors#####
#############################
#Recreating a dataset that has missing values.
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10)))
missing.data
#Imputing using 1NN.
imputed.1nn = kNN(missing.data, k=1)
imputed.1nn
?impute
missing.data
#Imputing using 1NN.
imputed.1nn = kNN(missing.data, k=1)
imputed.1nn
#Imputing using 1NN.
imputed.1nn = kNN(missing.data, k=1)
imputed.1nn
#Imputing using 5NN.
imputed.5nn = kNN(missing.data, k=5)
imputed.5nn
#Imputing using 9NN.
imputed.9nn = kNN(missing.data, k=9)
imputed.9nn
### Imputing with caret
### Note: knnImpute with caret::preProcess force normalization
#Imputing using 1NN.
pre.1nn = preProcess(missing.data, method = 'knnImpute', k=1)
imputed.1nn = predict(pre.1nn, missing.data)
### Imputing with caret
### Note: knnImpute with caret::preProcess force normalization
#Imputing using 1NN.
install.packages('RANN')
library(RANN)
pre.1nn = preProcess(missing.data, method = 'knnImpute', k=1)
imputed.1nn = predict(pre.1nn, missing.data)
#Imputing using 5NN.
pre.5nn = preProcess(missing.data, method = 'knnImpute', k=5)
imputed.5nn = predict(pre.5nn, missing.data)
#Imputing using 9NN.
pre.9nn = preProcess(missing.data, method = 'knnImpute', k=9)
imputed.9nn = predict(pre.9nn, missing.data)
imputed.1nn #Inspecting the imputed values of each of the methods;
imputed.5nn #what is going on here? Given our dataset, should we
imputed.9nn #expect these results?
#############################
#####K-Nearest Neighbors#####
#############################
#Recreating a dataset that has missing values.
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10)))
missing.data
#Imputing using 1NN.
imputed.1nn = kNN(missing.data, k=1)
imputed.1nn
#Imputing using 5NN.
imputed.5nn = kNN(missing.data, k=5)
imputed.5nn
#Imputing using 9NN.
imputed.9nn = kNN(missing.data, k=9)
imputed.9nn
### Imputing with caret
### Note: knnImpute with caret::preProcess force normalization
#Imputing using 1NN.
install.packages('RANN')
library(RANN)
pre.1nn = preProcess(missing.data, method = 'knnImpute', k=1)
imputed.1nn = predict(pre.1nn, missing.data)
#Imputing using 5NN.
pre.5nn = preProcess(missing.data, method = 'knnImpute', k=5)
imputed.5nn = predict(pre.5nn, missing.data)
pre.5nn2 = preProcess(missing.data, method = c('center', 'scale', 'knnImpute'), k=5)
imputed.5nn2 = predict(pre.5nn2, missing.data)
imputed.5nn
imputed.5nn2
pre.5nn2 = preProcess(missing.data, method = c('scale', 'knnImpute'), k=5)
imputed.5nn2 = predict(pre.5nn2, missing.data)
imputed.5nn
imputed.5nn2
#K-Nearest Neighbors regression on the sleep dataset.
sqrt(nrow(sleep)) #Determining K for the sleep dataset.
#Using 8 nearest neighbors.
pre.8nn = preProcess(sleep, method = 'knnImpute', k=8)
sleep.imputed8NN = predict(pre.8nn, sleep)
summary(sleep) #Summary information for the original sleep dataset.
summary(sleep.imputed8NN[, 1:10]) #Summary information for the imputed sleep dataset.
#K-Nearest Neighbors classification on the iris dataset.
help(iris) #Inspecting the iris measurement dataset.
iris
iris.example = iris[, c(1, 2, 5)] #For illustration purposes, pulling only the
#Throwing some small amount of noise on top of the data for illustration
#purposes; some observations are on top of each other.
set.seed(0)
iris.example$Sepal.Length = jitter(iris.example$Sepal.Length, factor = .5)
iris.example$Sepal.Width = jitter(iris.example$Sepal.Width, factor= .5)
col.vec = c(rep("red", 50), #Creating a color vector for plotting purposes.
rep("green", 50),
rep("blue", 50))
plot(iris.example$Sepal.Length, iris.example$Sepal.Width,
col = col.vec, pch = 16,
main = "Sepal Measurements of Iris Data")
legend("topleft", c("Setosa", "Versicolor", "Virginica"),
pch = 16, col = c("red", "green", "blue"), cex = .75)
missing.vector = c(41:50, 91:100, 141:150) #Inducing missing values on the Species
iris.example$Species[missing.vector] = NA  #vector for each category.
iris.example
col.vec[missing.vector] = "purple" #Creating a new color vector to
plot(iris.example$Sepal.Length, iris.example$Sepal.Width,
col = col.vec, pch = 16,
main = "Sepal Measurements of Iris Data")
legend("topleft", c("Setosa", "Versicolor", "Virginica", "NA"),
pch = 16, col = c("red", "green", "blue", "purple"), cex = .75)
#Inspecting the Voronoi tesselation for the complete observations in the iris
#dataset.
install.packages('deldir')
library(deldir) #Load the Delaunay triangulation and Dirichelet tesselation library.
info = deldir(iris.example$Sepal.Length[-missing.vector],
iris.example$Sepal.Width[-missing.vector])
plot.tile.list(tile.list(info),
fillcol = col.vec[-missing.vector],
main = "Iris Voronoi Tessellation\nDecision Boundaries")
#Adding the observations that are missing species information.
points(iris.example$Sepal.Length[missing.vector],
iris.example$Sepal.Width[missing.vector],
pch = 16, col = "white")
points(iris.example$Sepal.Length[missing.vector],
iris.example$Sepal.Width[missing.vector],
pch = "?", cex = .66)
#Conducting a 1NN classification imputation.
iris.imputed1NN = kNN(iris.example, k = 1)
#Assessing the results by comparing to the truth known by the original dataset.
table(iris$Species, iris.imputed1NN$Species)
#Conducting a 12NN classification imputation based on the square root of n.
sqrt(nrow(iris.example))
iris.imputed12NN = kNN(iris.example, k = 12)
#Assessing the results by comparing to the truth known by the original dataset.
table(iris$Species, iris.imputed12NN$Species)
##################################################
#####Using Minkowski Distance Measures in KNN#####
##################################################
install.packages('kknn')
library(kknn) #Load the weighted knn library.
#Separating the complete and missing observations for use in the kknn() function.
complete = iris.example[-missing.vector, ]
missing = iris.example[missing.vector, -3]
#Distance corresponds to the Minkowski power.
iris.euclidean = kknn(Species ~ ., complete, missing, k = 12, distance = 2)
summary(iris.euclidean)
iris.manhattan = kknn(Species ~ ., complete, missing, k = 12, distance = 1)
summary(iris.manhattan)
setwd("/Users/heloisadutcosky/Documents/NYC Data Science Academy/Projetos/Projeto 2 - Web Scraping/Project2/data_processing/2. Cleaning/data")
crypto = read.csv("crypto_date.csv", stringsAsFactors = F)
dj = read.csv("dow_jones.csv", stringsAsFactors = F)
ct = read.csv("cointelegraph_nlp.csv", stringsAsFactors = F)
rc = read.csv("reddit_nlp.csv", stringsAsFactors = F)
library(dplyr)
library(ggplot2)
library(ggthemes)
library(googleVis)
library(scales)
library(RColorBrewer)
library(plotly)
library(shiny)
library(shinydashboard)
library(htmlwidgets)
library(devtools)
library(DT)
library(tidyr)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message=FALSE)
# Your code here
install.packages('PASWR')
# Your code here
install.packages('PASWR')
library(PASWR)
library(dplyr)
library(dplyr)
# Columns with missing values:
missing_col = colnames(titanic3)[colSums(is.na(titanic3)) > 0]
# How many missing values:
colSums(is.na(titanic3))[missing_col]
library(mice) #Load the multivariate imputation by chained equations library.
na = mice::md.pattern(titanic3) #Can also view this information from a data perspective.
print(na)
sum(!complete.cases(titanic3))
mean(!complete.cases(titanic3))
sum(rowSums(is.na(titanic3))>0)
nrow(titanic3)
sum(is.na(titanic3))
(nrow(titanic3)*ncol(titanic3))
sum(is.na(titanic3)) / (nrow(titanic3)*ncol(titanic3))
library(Hmisc)
impute(titanic3$age, 'mean')
plot(hist(titanic3$age), col = NULL, border = "blue", add = T) +
plot(density(impute(titanic3$age, 'random')), col = "red", add = T)
plot(hist(titanic3$age), col = NULL, border = "blue", add = T) +
plot(density(impute(titanic3$age, 'random')), col = "red", add = T)
plot(hist(titanic3$age), col = NULL, border = "blue", add = T) +
plot(density(impute(titanic3$age, 'mean')), col = "red", add = T)
plot(hist(titanic3$age), col = NULL, border = "blue", add = T)
plot(density(impute(titanic3$age, 'mean')), col = "red", add = T)
# Your code here
library(Hmisc)
impute(titanic3$age, 'mean')
plot(hist(titanic3$age), col = NULL, border = "blue", add = T)
plot(density(impute(titanic3$age, 'mean')), col = "red", add = T)
plot(age.randomimpute, fare.randomimpute, col = col.vec)
legend("topleft", c("1st", "2nd", "3rd"),
pch = 1, col = c("red", "blue", "green"))
new.people = data.frame(age = c(50, 10), fare = c(400, 100), pclass = NA)
points(new.people, pch = 16)
## use ggplot2
imputed.titanic= as.data.frame(cbind(age.randomimpute,fare.randomimpute,pclass=titanic3$pclass))
crypto = read.csv("crypto_date.csv", stringsAsFactors = F)
crypto = select(crypto, -1)
library(dplyr)
crypto = select(crypto, -1)
library(dplyr)
library(ggplot2)
library(tidyr)
View(crypto)
crypto = select(crypto, -1)
crypto = select(crypto, -X)
View(crypto)
library(dplyr)
crypto = select(crypto, -X)
crypto = select(crypto, -1)
unique(rc$coin)
crypto$name[crypto$name == "XRP"] = "Ripple"
rc_crypto = left_join(rc, crypto, by = c("coin" = "name", "news_date" = "date"))
View(rc_crypto)
crypto = select(crypto, -1)
crypto = select(crypto, -X)
View(rc_crypto)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message=FALSE)
# Your code here
library(MASS)
cats
plot(x = cats$Bwt, y = cats$Hwt, add = T)
View(ct)
separate(ct, coins_list)
ct_test = separate(ct, coins_list, into = unique(rc$coin), sep = " ")
View(ct_test)
unique(ct$coins_list)
paste(unique(ct$coins_list))
install.packages("splitstackshape")
library(splitstackshape)
ct_test = cSplit_e(ct, coins_list, sep = " ")
ct_test = cSplit_e(ct, 'coins_list', sep = " ")
View(ct_test)
rm(ct_test)
ct_test = cSplit_e(ct, 'coins_list', sep = " ", mode= 'binary', type='character', fill=0, drop=t)
View(ct_test)
View(ct_test)
ct_test = cSplit_e(ct, 'coins_list', sep = " ", mode= 'binary', type='character', fill=0, drop=t) %>%
select(-coins_list)
crypto = select(crypto, -1)
crypto = select(crypto, -X)
library(dplyr)
library(ggplot2)
library(tidyr)
setwd("/Users/heloisadutcosky/Documents/NYC Data Science Academy/Projetos/Projeto 2 - Web Scraping/Project2/data_processing/2. Cleaning/data")
crypto = select(crypto, -X)
crypto = select(crypto, -1)
